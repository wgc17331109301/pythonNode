[风变爬虫知识库](https://docs.forchange.cn/docs/2wAlXK56optg8DAP)

[风变爬虫案例](https://docs.forchange.cn/docs/aBAYV4zKD1h2Yr3j)

[风变爬虫常见错误知识库](https://docs.forchange.cn/docs/16q8Mmn0rXTQE7k7)

# 一、requests

```python
import requests
```

**robots协议：**

主域名 + /robots.txt

### 1、参数介绍

- headers  请求头

  ```python
  headers = {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
      # 跳转的页面
      'Referer': 'https://www.douyin.com/',
      'cookie':''
  }
  ```

  > **作用：伪装成浏览器**
  >
  > headers请求头一般带入的变量有host、referer、cookie、user-agent 
  >
  > 
  >
  > user-agent是指浏览器信息
  >
  > host是指这个网站的主域名
  >
  > referer是指你从哪个网页访问来的
  >
  > cookie指的是你的登录信息	

- params 请求参数

  ```python
  params = {
      'spm': 'a231o.13503973.search.1',
      'key': '食品',
      'pid': 'mm_130402922_1108650181_109788200281'
      
  }
  ```

  > **作用：将参数放在url中传递**
  >
  > 
  >
  > https://ai.taobao.com/search/index.htm?spm=a231o.13503973.search.1&key=%E9%A3%9F%E5%93%81&pid=mm_130402922_1108650181_109788200281
  >
  > 相当于上面路由   ？号后面的参数

- 使用代理 ip

  ```python
  # 第一个http/https 是跟网站有关
  # 第二个是跟ip
  proxies = {
      "键名":"协议://IP地址:端口号"
    # "http": "http://115.152.235.186:4245",
    "https": "http://115.152.235.186:4245",
      
  }
  ```

  > 为了反爬

- data 参数

  ```python
  data ={
      'username':'q123',
      'password':'q123',
      'submit':None,
      'action': 'login'
  }
  ```

  > post 请求需要的参数，传一个表单数据给服务器

- 其它常用参数

  | 参数    | 说明                                                         | 使用示例       |
  | ------- | ------------------------------------------------------------ | -------------- |
  | verify  | 如果SSL证书验证不通过，或者不信任服务器的安全证书，<br>需要加入此参数，默认True开启 | verify = False |
  | timeout | 超时参数，超过时间就会停止（秒）                             | timeout = 3    |


### 2、发起请求

**语法：request.请求方法(路由url，参数设置)**

```python
# get请求
res = request.get(url,headers= headers,params=params)

# post请求
res = request.post(url,headers=headers,data=data)
```

> 请求方法：常见的有get、post、put、delete等等
>
> 
>
> get：发送请求来获得服务器上的资源，请求体中不会包含请求数据
>
> post：向服务器提交资源让服务器处理，比如提交表单、上传文件等

- session 保持登录状态

  ```python
  session_ = requests.session()  # 得到一个对象
  # session既然是通过requests进行构建的,,同样可以发送网络请求,,添加了新的功能
  # 保持登录状态,自动携带cookie
  # 只要利用session对象发送了post请求模拟登录,,自动携带登录后的cookie
  session_.post(url, headers=headers, data=data)
  ```

  > session是通过requests进行构建的,同样可以发送网络请求,添加了新的功能
  >
  > 保持登录状态,自动携带cookie
  >
  > 
  >
  > 只要利用session对象发送了post请求模拟登录,,自动携带登录后的cookie

**常见响应状态码：**

| 响应状态码 | 说明       | 说明           |
| ---------- | ---------- | -------------- |
| 1xx        | 请求收到   | 继续提出请求   |
| 2xx        | 请求成功   | 成功           |
| 3xx        | 重新定向   | 应使用代理访问 |
| 4xx        | 客户端错误 | 无法提供信息   |
| 5xx        | 服务器错误 | 服务不可用     |

**HTML常见标签：**

| 标签             | 描述                                   |
| ---------------- | -------------------------------------- |
| <p>              | 段落元素                               |
| <h1>,<h2>...<h6> | 标题元素                               |
| <html>           | 定义一个HTML文档                       |
| <head>           | 设置文档标题和其它在网页中不显示的信息 |
| <title>          | 设置文档标题                           |
| <body>           | 设置主题内容                           |
| <br>             | 换行                                   |
| <a>              | 超链接                                 |
| <ul>             | 无序列表                               |
| <ol>             | 有序列表                               |
| <li>             | 列表项                                 |
| <dl>             | 自定义列表                             |
| <dt>             | 自定义列表项                           |
| <dd>             | 自定义列表对应的注释                   |
| <div>            | 定义文档中的分区                       |

**常见属性：**

| 属性  | 描述              |
| ----- | ----------------- |
| href  | 规定链接的目标URL |
| id    | 规定元素的唯一id  |
| class | 规定元素的类名    |

### 3、获取响应内容

**语法：res响应对象.类型方法**

```python
# 二进制，字节数据（bytes类型）
res.content

# 文本数据（str类型）
res.text

# json数据
res.json()
```

> content：原始二进制字节流，可以用来保存图片等二进制文件
>
> 
>
> text：文本编码自动解码响应内容，大多数 Unicode 字符集都能被无缝地解码。
>
> 
>
> **json简介：列表嵌套字典的形式，字符串类型， 是一种轻量级的数据交换格式。**
>
> json：将响应json数据转化为Python字典 注意不能是HTML类型的响应

- **其它方法**

- **语法：响应对象 . 方法名 **

  | 参数                | 说明                                     |
  | ------------------- | ---------------------------------------- |
  | **status_code**     | **响应状态码**                           |
  | **request.headers** | **响应对应的请求头**                     |
  | **headers**         | **响应头**                               |
  | **request.cookies** | **请求的cookie**                         |
  | **cookies**         | **响应的cookie（经过了set-cookie动作）** |
  | **encoding**        | **设置编码格式**                         |

  ```python
  # 设置编码格式
  res.encoding = 'utf-8'
  ```

### 4、附：报错继续调用

方法一：

```python
def fun():
    time.sleep(1)
    try:
        raise Exception('出错了')
    except Exception as e:
        print(e)
        fun()
fun()
```

方法二：

```python
# 导包
from retrying import retry

@retry()
def fun():
    time.sleep(1)
    print('------出错了----')
    raise Exception('出错了')

fun()
```

### 6、动态网页

```
动态网页XHR文件中找
AJAX
```

### 7、爬虫模板

```python
#导入相关模块库
import requests
from bs4 import BeautifulSoup
import csv

#设置存储爬取数据的列表，方便后面写入到csv文件中
data_list = []

#01获取网页
url = ''
#设置请求头
headers = {}
res = requests.get(url,headers = headers)
#打印状态码是否正常
print(res.status_code)

#02解析网页
bs = BeautifulSoup(res.text,'html.parser')
#根据需求使用find() or find_all()方法查找标签元素
Tag = bs.find()
print(Tag.text)
Tag_list = bs.find_all()
for i in Tag_list:
    data1 = i.find().text
    data2 = i.find().text
    #设置一个字典存储爬取的数据，方便之后添加到data_list里面
    data_dict = {
        '表头数据1':data1,
        '表头数据2':data2
    }
    #检查爬取的信息
    print(data_dict)
    #存储爬取信息到dat_list里面
    data_list.append(data_dict)

#03存储网页
with open('xxxx.csv', 'w', encoding='utf-8-sig') as f:
    # 将文件对象转换成 DictWriter 对象，并定义表头数据
    f_csv = csv.DictWriter(f, fieldnames=['表头数据1', '表头数据2'])
    # 写入表头与数据
    f_csv.writeheader()
    f_csv.writerows(data_list)
```

# 二、selenium

```python
1）Windows 系统：pip install selenium
2）Mac 系统：pip3 install selenium

Chrome 浏览器驱动配置流程: https://docs.forchange.cn/docs/9030MKLRVRtV2Oqw
```

### 1、selenium的使用

```python
# 导包
from selenium import webdriver
# 实例谷歌浏览器对象
driver = webdriver.Chrome()
```

### 2、基本方法

| get(url)              | 打开浏览器                   |
| --------------------- | ---------------------------- |
| current_url           | 返回url                      |
| page_source           | 获取网页源代码               |
| maximize_window()     | 浏览器最大化                 |
| save_screenshot()     | 页面截图                     |
| window_handles        | 获取所有句柄                 |
| current_window_handle | 获取当前句柄                 |
| get_cookies()         | 获取当前网页的Cookie（列表） |
|                       |                              |
| close()               | 关闭浏览器                   |
| quit()                | 退出                         |

```python
# 打开浏览器
driver.get("https://www.baidu.com/)
# current_url 返回当前url
print(driver.current_url)
# page_source 打印网页源代码
print(driver.page_source)
# 浏览器最大化
driver.maximize_window()
# 页面截图 
# driver.save_screenshot('chaojiying.jpg')
# window_handles 获取所有的句柄
print(driver.window_handles)
# 获取当前的句柄
print(driver.current_window_handle)

# 你页面打开了新的标签页 并不会操作新的标签页
# 通过句柄的操作来切换标签页
driver.switch_to.window(driver.window_handles[1])
# 获取当前页的标题
print(driver.title)

# 进入iframe之后才能获取iframe里面的元素
# switch_to.frame 进入到iframe框架中
driver.switch_to.frame('g_iframe')
```

### 3、定位点击操作

##### （1）定位元素

**注意：可多层查找**

| 方法                                | 说明                                                         |
| ----------------------------------- | ------------------------------------------------------------ |
| find_element_by_name()              | 通过属性 **name** 的值查找                                   |
| find_element_by_class_name()        | 通过属性 **class** 的值查找                                  |
| find_element_by_id()                | 通过属性 **id** 的值查找                                     |
| find_element_by_xpath()             | 通过 **xpath** 查找                                          |
| find_element_by_css_selector()      | 通过 **CSS选择器**  查找                                     |
| find_element_by_tag_name()          | 通过 **标签名称** 查找                                       |
| find_element_by_link_text()         | 通过  **超链接文本**  查找                                   |
| find_element_by_partial_link_text() | 通过 **超链接的部分文本**  查找                              |
| 以上方法element后面加s              | 查找多个元素，返回符合条件的对象列表<br>（没有则返回空列表） |

```python
# 定位到指定位置输入账号
driver.find_element_by_id('freename')

# 定位到指定位置输入密码
driver.find_element_by_class_name('password')

# 定位到登录位置
ele = driver.find_element_by_class_name('loginBtn')

# 1、click 点击登录
ele.click()

# 2.通过js语法进行点击指定元素ele
# js进行元素点击语法 arguments[0].click 相当于ele.click()
driver.execute_script('arguments[0].click()',ele)
```

##### （2）输入、点击操作

**下方enement 代表定位到的元素**

| 方法                          | 说明                                 |
| ----------------------------- | ------------------------------------ |
| element.send_keys(“内容”)     | **输入**内容                         |
| element.click()               | **点击**此元素                       |
| element.screenshot_as_png     | 截取定位到的元素内容（**截图**）     |
| element.get_attribute("属性") | **获取**定位到的元素的**属性**       |
| element.text                  | **获取**定位元素下的所有**文本内容** |
|                               |                                      |
|                               |                                      |
|                               |                                      |
|                               |                                      |

```python
# 定位到登录位置
ele = driver.find_element_by_class_name('loginBtn')

# 输入用户名
ele.send_keys(input('输入用户名'))

# 点击登录
ele.click()



# 通过element.text获取内容
ele.text
# 通过get_attribute获取属性
ele.get_attribute('href')
```



### 4、页面滚动

| 执行像素单位进行移动     | window.scrollTo(0,2000)# (横向x轴,竖向y轴)    |
| ------------------------ | --------------------------------------------- |
| 可以以页面为单位进行移动 | window.scrollTo(0,document.body.scrollHeight) |
| 指定到达的坐标           | document.documentElement.scrollTop=2000       |

```python
chrome_obj.execute_script('document.documentElement.scrollTop=2000')
```

### 5、无头浏览与反检测

方法一：

```python
from selenium.webdriver.chrome.options import Options

# 实例化一个对象
options = Options()
# 给对象添加属性 不需要打开浏览器就能使用
options.add_argument('--headless')

# 把刚刚添加好的无头模式 给driver对象添加上
driver = webdriver.Chrome(options=options)
```

方法二：

```python
options.headless = True
```

方法三：

```python
options.set_headless()
```

**反检测：**

```python
# 添加参数,进行隐藏
options.add_experimental_option('excludeSwitches', ["enable-automation"])  # 不需要死记硬背
chrome_obj = Chrome(options=options)
```

### 6、显示等待与隐士等待

导包：

```python
# 规定导包下面三个
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
```

显示等待：

```python
# 显示等待  要等到某个元素加载完了之后才继续往后执行
#           设置10秒 10秒内提前加载好了后就不会等10秒 提前执行

# 显示等待  等待元素加载完成 肯定需要一个元素
# 第一个参数 driver 就是实例化的driver
# 第二个参数 timeout 等待的时间
# 第三个参数 poll_frequency 监测时间 默认每过0.5秒监测一次 直到元素已经加载完成
# EC.presence_of_element_located(By.XPATH) 通过xpath来获取元素
# EC.presence_of_element_located 这个方法里面需要放元祖
# 等待元素10秒内加载完成 每过1秒查看一下
ele = WebDriverWait(driver,10,1).until(EC.presence_of_element_located((By.XPATH,'//*[@id="1"]/h3/a')))
# ele = WebDriverWait(driver,10,1).until(EC.presence_of_element_located((By.ID,'//*[@id="1"]/h3/a')))
# ele = WebDriverWait(driver,10,1).until(EC.presence_of_element_located((By.CLASS_NAME,'//*[@id="1"]/h3/a')))
```

隐士等待：

```python
# 隐示等待 等待页面加载完成 加载完了之后就继续往后面执行 灵活 提前加载好提起执行
# 有多个页面 多个标签页

# 设置隐示等待 页面加载完 继续往后面执行
driver.implicitly_wait(0.1)
```

### 7、高级滑动操作

```python
from selenium.webdriver import ActionChains

# 1.找到这个按钮
click_obj = chrome_obj.find_element_by_xpath('/html/body/div/div/div/div[1]/div[1]/div[3]')

# 2.按住:动作(需要使用动作链)
action_obj = ActionChains(chrome_obj)  # 参数是浏览器对象,
action_obj.click_and_hold(click_obj)  # 点击并且按住,参数为具体的节点(按钮)

# 3.滑动(x横向移动,y竖向移动) 在这之前可以得到他的长和宽
action_obj.move_by_offset(100, 0).perform()
#            定位                 #  启动

# 4.松开点击
action_obj.release()  # 释放
```

# 三、js加密

### 1、分析

```python
目的: 爬虫实现有道翻译: 输入一个数据(参数), 得到对应的翻译结果(响应数据)
1.分析数据包:参数的传递,get请求url传参,post请求里面的form表单传参

2.对应的数据包:
    --post请求:需要发送请求的时候,携带form表单
    --json格式:响应数据是json格式的数据,jsonpath

3.确认目标的url:http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule

4.检查form表单里面的参数,更换了要被数据之后,,表单里面的键值对是否会跟着发生改变

5.进行另一次请求,,对比参数的不同
    --没有发生改变,直接复制写死就好
    --发生了改变的参数,是怎么发生改变,(分析:重点)

6.form表单里面发生改变的参数
    -- √ i: 要被翻译的数据
    -- salt: 16178019691804 > 16178031806006
    -- sign: 126c8264337c5edec47981a61d039159 > 52e6d5069cf6d2d63b56419e729b73ee
    -- √lts: 1617801969180 > 1617803180600
    总结:发生了变化的数据:一共是四个

7.lts: 由纯数字组成,salt是lts在末尾加了一位数字的结果
    --纯数字组成,是否是一个时间戳数据
    结论:以毫秒为单位的时间戳

  salt: 以毫秒为单位的时间戳, 末尾加上一位数字(未知规律逻辑)
```

### 2、python运行js代码

安装：pip install PyExecJS

```python
# 导入: 使用不是同一个名字 execjs
import execjs

# 拿到js代码
with open('demo.js','r',encoding='utf-8') as f:
    js_data = f.read()

# 转换一下类型,转换操作
js_obj = execjs.compile(js_data)

# 执行
res_ = js_obj.call('add',6,5)
print(res_)
```

# 四、fiddle抓包工具

### 1、fiddle设置

```python
fiddler的一个初始设置:
1.最上方菜单栏的工具栏 Tools 点击第一个选项 options
2.出现options的菜单栏,选择第二个HTTPs,默认不支持https协议数据包的获取,
3.勾选DEcryPThttps,如果是第一次,提示你安装证书,next install,,success...
4.点击options的菜单栏里面的第三个选项connection
5.第二个勾选 allow remote computers to conect
6.重启
```

### 2、与移动端建立连接

```python
移动端客户端:
1.手机
2.手机模拟器(虚拟机之于电脑)
    -- 夜神模拟器   
    -- 腾讯手游助手
    -- mumu模拟器  网易
"""
"""
1.刚刚下载好这个手机模拟器:
抓包工具是否会自动的存在于 移动客户端和 服务器之间.
怎么样才能够让彼此独立分开的软件, 存在 移动端客户端 >发送请求> 抓包工具fiddler >   服务器这样一个关系

原理: 移动端客户端发送网络请求, 必须依赖抓包工具, 才能发送给服务器
1.知道自己电脑的IP地址:192.168.1.150(举例)
2.知道抓包工具的端口号:fiddler 默认8888
3.手机模拟器设置:
    -- 找到设置,网络,按住不动,出现修改网络
    -- 代理改成电脑IP 192.168.1.150
    -- 端口8888
4.手机模拟器发送网络请求,有需要依赖指定IP的指定端口8888的工具fiddler
5.手机模拟器发送网络请求,只能通过fiddler
6.手动模拟器  >  fiddler  > 服务器
7.如果你的fiddler没有打开运行,,,那么手机模拟器就无法发送网络请求
8.需要去下载一个证书:
    --打开浏览器:网址栏输入ip:port
    --download fiddler证书
    --证书命名,点击保存
9.安装证书 > 设置一个开屏密码
```

### 3、使用步骤

```python
1.打开一个app
2.清空数据包
3.发送请求
4.暂停抓包
5.数据包的类型, body的大小
6.点击一个数据包,选择inspetor
    --上面部分是request headers的信息
    --下面部分是response header的信息
7.通过响应数据查看是否是对应的数据包
    --在response header部分选择webView 相当于network里面的response部分
    -- body:21 {"result":{},"ret":0}
8.在 request headers部分点击Raw的部分:
   还是headeres部分,,,都存在请求的一些参数,,信息
   --url:
   --get
   --cookie
   --User-Agent
9.对应的数据包右键点击: justurl
    -- https://mlol.qt.qq.com/go/mlol_news/recommend_feeds?main_feeds=1&plat=android&ip=10.0.2.15&network=wifi&favzone=lol&zone=plat&channel=1&transport=&disableAlgo=%24DISABLEALGO%24&subChannel=1&slidetype=0&next=&version=10502 
```

# 五、scrapy框架

[Scrapy文档地址](http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html)

### 1、概念和流程

- 流程![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg2018.cnblogs.com%2Fblog%2F1519076%2F201903%2F1519076-20190309141306862-221827008.png&refer=http%3A%2F%2Fimg2018.cnblogs.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1650514220&t=ccb4c3d95f7a9f49397755cc9fe2cf96)

  > 流程描述：
  >
  > 1. 爬虫中起始的url构造成request对象-->爬虫中间件-->引擎-->调度器
  > 2. 调度器把request-->引擎-->下载中间件--->下载器
  > 3. 下载器发送请求，获取response响应---->下载中间件---->引擎--->爬虫中间件--->爬虫
  > 4. 爬虫提取url地址，组装成request对象---->爬虫中间件--->引擎--->调度器，重复步骤2
  > 5. 爬虫提取数据--->引擎--->管道处理和保存数据

- 每个模块的具体作用

  ![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg-blog.csdnimg.cn%2F20200222134056362.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0JpYW5fWmh1YW5n%2Csize_16%2Ccolor_FFFFFF%2Ct_70&refer=http%3A%2F%2Fimg-blog.csdnimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1650514220&t=85ded70bc573fd475326ffcbfc6f63f4)

- **scrapy基本组成**

  ```python
  --中间部分: 引擎组件
  --上面部分: 调度器
  --下面部分: 爬虫器,爬虫组件
  --左边部分: 管道组件
  --右边部分: 下载器组件
  ```

- **scrapy步骤**

  ```python
  引擎组件:领导,核心
      2.引擎拿着爬虫组件给的request请求对象,交给了调度器
      4.引擎拿着调度器排序之后的request请求对象,交给下载器
      6.引擎拿着下载器给的响应对象response,交给了爬虫组件
      8.引擎拿着爬虫组件发送过来的数据:
          --如果是请求对象request,把之前的步骤重新执行一遍
          --如果是需要保存的数据,就交给管道组件,item对象
  
  爬虫组件:1.确认起始的url: 爬虫组件会根据起始的url构造一个请求对象request,把请求对象request交给引擎
           7.拿着引擎给的响应对象response,进行数据的提取:
              --如果提取出来的是需要保存的数据data,那么就需要进行保存   item对象  交给引擎
              --如果提取到的url,是需要继续发送网络请求的,构造一个请求对象request  交给引擎
  
  调度器:3.拿着从引擎给的request请求对象进行排序(入队列,出队列),把排序之后的request请求对象交给引擎
  
  下载器:5.拿着从引擎给的request请求对象,发送网络请求,获取响应对象response,交给引擎
  
  管道组件:9.拿着要被保存的数据,保存起来
  ```

### 2、入门使用

- 安装

  ```python
  命令:     sudo apt-get install scrapy 或者：     pip/pip3 install scrapy
  ```

- 项目开发流程

  ```
  1. 创建项目:        scrapy startproject sixstartText
  2. 生成一个爬虫:    scrapy genspider sixtartText sixstaredu.com
  3. 提取数据:       根据网站结构在spider中实现数据采集相关内容
  4. 保存数据:       使用pipeline进行数据后续处理和保存
  ```

  > 注意：创建项目后需要cd 到项目中才可进行下一步

- 基础命令

  ```python
  1、创建项目： scrapy    startproject    xxx(项目名称)   创建项目
  
  2、1 创建文件名称（默认basic模板）： scrapy  genspider   xxx(爬虫任务名称)    www.com（域名）
  
  2、2 创建文件名称（crawl模板 -- 自动提取url）：
  		scrapy   genspider  -t  crawl    xxx     www.com
      
  
  3、开启爬虫： scrapy   crawl  xxx(文件名称) --nolog(无日志，报错的话就需要取消查看报错信息)
  
  4、进入监听状态：scrapy    runspider   xxx.py       
               这个详细查看分布式爬虫步骤.py
      
  bench : scrapy bench 快速的测试一下框架运行起来的样子
  
  fetch : scrapy fetch url 拿一个url进行测试 对展示出对应的响应
  
  genspider: 创建爬虫任务用的
  
  runspider: 运行爬虫项目
  
  shell: 快速的进行一些代码的测试: 测试部分代码的执行的时候使用 svrapy shell 起始url
  
  
  version: 显示版本
  
  view: view视图 scrapy view https://www.baidu.com
       快速的测试一个url的响应的获取,渲染出来,为了检验成功与否
  ```

##### 各个文件介绍：

- settings

  ```python
  BOT_NAME = 'scrapy_sample'     # 爬虫项目的名称
  
  SPIDER_MODULES = ['scrapy_sample.spiders']     # 爬虫应用的模板
  NEWSPIDER_MODULE = 'scrapy_sample.spiders'     # 爬虫应用的模板
  
  
  # 设置全局的用户代理User -Agent    **
  USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
  
  
  ROBOTSTXT_OBEY = False    # robots协议，需要手动改成False  **
  
  
  # CONCURRENT_REQUESTS = 32     # 并发请求数的设置
  
  
  DOWNLOAD_DELAY = 3    # 下载延迟    **
  
  #CONCURRENT_REQUESTS_PER_DOMAIN = 16   # domain  单域名访问并发数
  #CONCURRENT_REQUESTS_PER_IP = 16       # 单IP的并发数
  
  
  COOKIES_ENABLED = False    # 全局的Cookie的设置  **
  
  
  #TELNETCONSOLE_ENABLED = False   # 查看爬虫信息相关
  
  
  # 设置默认全局的请求头    **
  DEFAULT_REQUEST_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'en',
    'Cookie':'xxx'
  }      
  
  
  # 爬虫中间件，注释掉了就不会生效
  SPIDER_MIDDLEWARES = {
     # 'scrapy_sample.middlewares.ScrapySampleSpiderMiddleware': 543,
  }        
  
  
  # 下载器中间件，  UA,Cookie，Referer的设置   **
  DOWNLOADER_MIDDLEWARES = {
     # 'scrapy_sample.middlewares.ScrapySampleDownloaderMiddleware': 543,
      # selenium中间件管道的开启
     'scrapy_sample.middlewares.seleniumDownloaderMiddleware': 544,
  }         
  
  
  # 自定义爬虫信息
  #EXTENSIONS = {
  #    'scrapy.extensions.telnet.TelnetConsole': None,
  #}    
  
  
  # 管道（用来保存逻辑的部分）    路径             权重（权重越小，优先级越大） **
  ITEM_PIPELINES = {
     'scrapy_sample.pipelines.ScrapySamplePipeline': 300,
     'scrapy_sample.pipelines.ScrapySamplePipeline2': 301,
     'scrapy_sample.pipelines.images.ImagesPipeline' : 302  # 图片管道路径
  } 
  
  
  #AUTOTHROTTLE_ENABLED = True    # 自动限速
  
  #AUTOTHROTTLE_START_DELAY = 5   # 初始下载延迟的设置
  
  #AUTOTHROTTLE_MAX_DELAY = 60    # 最大下载延迟
  
  #AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0    # 设置每秒并发数
  
  #AUTOTHROTTLE_DEBUG = False    # 是否显示DEBUG
  
  
  #HTTPCACHE_ENABLED = True             # 缓存设置
  #HTTPCACHE_EXPIRATION_SECS = 0        # 缓存时间
  #HTTPCACHE_DIR = 'httpcache'          # 路径
  #HTTPCACHE_IGNORE_HTTP_CODES = []     # 缓存http的状态码
  #HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'   # 缓存插件
  ```
  
- 创建的爬虫文件

  ```python
  import scrapy
  
  
  class TextSpider(scrapy.Spider):       # 继承自Spider这个类
      name = 'text'    # 爬虫任务名称
      allowed_domains = ['www.com']     # 允许请求的域的范围
      start_urls = ['http://www.com/']  # 起始url，从这个网站开始爬取
  
      def parse(self, response):   # 用来做数据的提取，响应对象response
          pass
  ```

  > 主要用来提取文件

- items.py

  ```python
  import scrapy
  
  
  # 预处理，这个类需要在Spider的py文件中导包才能使用
  # 若要在爬虫文件中使用，需要提前在此创建好字段
  class ScrapySampleItem(scrapy.Item):
     
      # name = scrapy.Field()
      xxx = scrapy.Field()
      xxxx = scrapy.Field()
      xxxxx = scrapy.Field()
      pass
  ```

- middlewares（中间件）.py

  ```python
  import random
  import time
  
  from scrapy import signals
  
  
  from itemadapter import is_item, ItemAdapter
  from scrapy.http import HtmlResponse
  
  
  class ScrapySampleSpiderMiddleware:
      @classmethod
      def from_crawler(cls, crawler):
          # This method is used by Scrapy to create your spiders.
          s = cls()
          crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
          return s
  
      def process_spider_input(self, response, spider):
  
          return None
  
      def process_spider_output(self, response, result, spider):
  
          for i in result:
              yield i
  
      def process_spider_exception(self, response, exception, spider):
          pass
  
      def process_start_requests(self, start_requests, spider):
          for r in start_requests:
              yield r
  
      def spider_opened(self, spider):
          spider.logger.info('Spider opened: %s' % spider.name)
  
  
  class ScrapySampleDownloaderMiddleware:
  
      @classmethod
      def from_crawler(cls, crawler):
          # This method is used by Scrapy to create your spiders.
          s = cls()
          crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
          return s
  
      # 发送请求携带的UA等在这里设置，需要在管道内开启
      # 处理请求对象的方法
      def process_request(self, request, spider):
          # 用户代理 User-Agent              随机选取
          request.headers['User-Agent'] = random.choice('构建的 UA 池')
  
          # 代理IP：https请求https代理IP，http请求http代理IP
          #        以https://moxie.douban.com / top250为例
          if request.url.split(':')[0] == 'http':   # 判断url是否是前缀http
              request.meta['Proxy'] = 'http://xxx.xxx.xxx.xxx:xxx'
          else:     # https
              request.meta['Proxy'] = 'http://xxx.xxx.xxx.xxx:xxx'
  
          # Cookie  （1、settings里面Cookie设置为True
          #           2、以键值对方式）
          #      以douban - fav - remind  == 1 ; __utmc = 3014；
          request.cookies = {"douban - fav - remind":"1","__utmc":"3014"}
  
          # 添加跳转Referer
          referer_url = request.url
          if referer_url:  # 拥有
              request.headers['Referer'] = referer_url
  
          return None
  
      def process_response(self, request, response, spider):
          return response
  
      def process_exception(self, request, exception, spider):
  
          pass
  
      def spider_opened(self, spider):
          spider.logger.info('Spider opened: %s' % spider.name)
  
  
  
  '''        selenium 使用下载中间件方法   使用需要在settings中开启下载中间件管道  '''
  class seleniumDownloaderMiddleware:
      def process_response(self, request, response, spider):
          # 判断是否是目标网址 ， 如果是 ，进行selenium请求
          if request.url in ['https://music.163.com/#/discover/playlist']:
              # 打开目标网址
              spider.driver.get(url=request.url)
              time.sleep(4)
  
              # 进入到  iframe  中
              spider.driver.switch_to.frame('g_iframe')
              # 通过page_source获取elenments 源代码
              time.sleep(4)
              row_response = spider.driver.page_source
              # current_url是selenium中返回url的方法
              return HtmlResponse(url=spider.driver.current_url,body=row_response,encoding='utf8',request=request)
  
          else:      # 如果上面代码没有执行，就执行下面代码
              return response
  ```
  
- pipelines（管道）.py

  ```python
  from itemadapter import ItemAdapter
  
  
  class ScrapySamplePipeline:
      def __init__(self):    # 自动调用只需要开启一次
          self.f = open('sixstar.txt', 'a', encoding='utf-8')
  
      def process_item(self, item, spider):       # 保存数据
          self.f.write(str(item) + '\n' + '\n')
          return item
  
      def __del__(self):        # 关闭，程序运行完毕自动执行
          self.f.close()
  ```

  > 管道：用来保存文件，需要在setting中开启管道

### 3、开发步骤详解

1. 创建项目

   ```
   scrapy startproject <项目名字> 
   ```

   > 生成的目录和文件结果如下：
   >
   > item.py：自己预计需要爬取的内容
   >
   > middlewares.py：自定义中间件的文件
   >
   > pipelines.py：管道，保存数据
   >
   > settings.py：设置文件，UA，启动管道
   >
   > spiders：自己定义的spider的文件夹
   >
   > scrapy.cfg：项目的配置文件

2. 创建爬虫文件

   ```
   scrapy genspider <爬虫名字> <允许爬取的域名>
   ```

   > 爬虫名字: 作为爬虫运行时的参数 允许爬取的域名: 为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果爬取的url与允许的域不通则被过滤掉。 
   >
   > 
   >
   > 注：需要先cd 到项目中

3. 提取数据

   **在/myspider/myspider/spiders/sixtartText.py中修改内容如下:**

   ```python
   import scrapy
   # 导入 items.py 中的  BasicItem类
   from ..items import BasicItem
   
   
   class SixstarSpider(scrapy.Spider):
       name = 'sixstar'
       allowed_domains = ['sixstaredu.com']
       start_urls = ['https://www.sixstaredu.com/teacher?page=%d'%a for a in range(1,5)]
   
       
       
       # 数据提取方法，接受下载中间件传过来的response
       def parse(self, response):
           # 获取教师名字
           names = response.xpath('//*[@id="content-container"]/div/div[*]/div/div[1]/h3/a/text()').getall()
           # 获取教师头衔
           positions = response.xpath('//*[@id="content-container"]/div/div[*]/div/div[1]/div/text()').getall()
           
   
           for i in range(len(names)):
               # 实例  items.py 的类
               item = BasicItem()
               # 构造字典 ， 键名分别是name ，position ，url
               item['name'] = names[i]
               item['position'] = positions[i].strip()
   
               # 获取详情页数据     url          回调函数     meta构造字典传给下面函数
               yield scrapy.Request((item['url']),callback=self.parse_data,meta={'jiuge':item})
   
               
       def parse_data(self,response):
           # 获取详情页的数据
           data = response.xpath('//*[@id="content-container"]/div/p//text()').getall()
   
   
           # 接收 meta 中的item
           item = response.meta['jiuge']
           item['data'] = ''.join(data)
   
           # 返回给管道
           yield item
   ```
   
   > 步骤：
   >
   > 1、在parse函数下，直接使用response.xpath方法即可提取数据
   >
   > CSS选择器提取数据：（也可以使用response.css(.属性)）
   >
   > getall：提取多个数据（列表），get：提取单个数据
   >
   > 2、实例items.py 中的  BasicItem类，构造字典数据（用什么字段需要先在BasicItem 类中添加）
   >
   > ```python
   > class BasicItem(scrapy.Item):
   >  # define the fields for your item here like:
   >  name = scrapy.Field()
   >  position = scrapy.Field()
   > ```
   >
   > 3、使用回调函数提取详情页数据
   
   - response 常用方法
   
     ```python
     response.url：当前响应的url地址
     response.request.url：当前响应对应的请求的url地址
     response.headers：响应头
     response.requests.headers：当前响应的请求头
     response.body：响应体，也就是html代码，byte类型
     response.status：响应状态码
     ```
   
4. 保存数据

   ```python
   1、在pipelines.py 中
   
   import json
   class ItcastPipeline():   
       # 爬虫文件中提取数据的方法每yield一次item，就会运行一次   
       # 该方法为固定名称函数   
       def process_item(self, item, spider):       
       print(item)       
       return item
   
   2、在settings.py配置启用管道
   
   ITEM_PIPELINES = {   
       'sixstart.pipelines.SixstartPipeline': 300,
   }
   
   ```

   > 配置项中键为使用的管道类，管道类使用.进行分割，第一个为项目目录，第二个为文件，第三个为定义的管道类。 
   >
   > 配置项中值为管道的使用顺序，设置的数值约小越优先执行，该值一般设置为1000以内。

5. 运行爬虫程序

   ```python
   1、运行方法一
   #  在项目目录终端输入scrapy    crawl    xxx(文件名)
   
   
   2、运行方法二
   在项目目录下新建main.py 文件夹
   #  导入 cmdline模块，可以实现控制终端命令行
   from scrapy import cmdline
   
   cmdline.execute(['scrapy','bench'])
   ```

### 4、crawl 模板的使用

```python
作用：crawlspider爬虫可以按照规则自动获取连接
```

> 与默认模板只在获取数据时不同

1. 创建爬虫文件

   ```python
   scrapy genspider -t crawl xxx(文件名)    xxx.com(域名)
   ```

2. 提取数据

   ```python
   import scrapy
   from scrapy.linkextractors import LinkExtractor
   from scrapy.spiders import CrawlSpider, Rule
   
   
   class TextCrawlSpider(CrawlSpider):
       name = 'text_crawl'
       allowed_domains = ['www.com']
       start_urls = ['http://www.com/']
   
       # url提取器               
       rules = (
           Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
       )
       # LinkExtractor参数
       # allow = (),   url范围，正则表达式
       # deny = (),     和上一个相反，除了上一个的范围都提取
       # allow_domains = (),     允许提取的域的范围
       # deny_domains = (),      同上相反
       # restrict_xpaths = (),     通过xpath提取
       # tags = ('a', 'area'),     默认提取a标签
       # attrs = ('href',),        默认提取href属性所对应的值
   
       def parse_item(self, response):
           item = {}
           #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
           #item['name'] = response.xpath('//div[@id="name"]').get()
           #item['description'] = response.xpath('//div[@id="description"]').get()
           return item
   ```

   > 注意：
   >
   > 1. 除了用命令scrapy genspider -t crawl <爬虫名> <allowed_domail>创建一个crawlspider的模板，也可以手动创建
   > 2. crawlspider中不能再有以parse为名的数据提取方法，该方法被crawlspider用来实现基础url提取等功能
   > 3. Rule对象中LinkExtractor为固定参数，其他callback、follow为可选参数
   > 4. 不指定callback且follow为True的情况下，满足rules中规则的url还会被继续提取和请求
   > 5. 如果一个被提取的url满足多个Rule，那么会从rules中选择一个满足匹配条件的Rule执行
   >
   > 
   >
   > Rule常见参数：：：：：
   >
   > LinkExtractor: 链接提取器，可以通过正则或者是xpath来进行url地址的匹配
   >
   > callback: 表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理
   >
   > follow: 连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，默认True表示会，Flase表示不会
   >
   > process_links: 当链接提取器LinkExtractor获取到链接列表的时候调用该参数指定的方法，这个自定义方法可以用来过滤url，且这个方法执行后才会执行callback指定的方法

### 5、scrpay-redis 分布式爬虫

- 概念

  1. 分布式是什么

     ```
     简单的说 分布式就是不同的节点（服务器，ip不同）共同完成一个任务
     ```

  2. scrpay-redis的概念

     ```
     scrapy_redis是scrapy框架的基于redis的分布式组件
     ```

  3. scrapy_redis作用

     ```
     Scrapy_redis在scrapy的基础上实现了更多，更强大的功能，具体体现在：
     	通过持久化请求队列和请求的指纹集合来实现：
     		1、断电续爬
     		2、分布式快速抓取
     ```

  4. 流程

     ```
     1、在scrapy_redis中，所有的待抓取的request对象和去重的request对象指纹都存在所有的服务器公用的redis中
     2、所有的服务器中的scrapy进程公用同一个redis中的request对象的队列
     3、所有的request对象存入redis前，都会通过该redis中的request指纹集合进行判断，之前是否已经存入过
     4、在默认情况下所有的数据会保存在redis中
     ```

1. 实现scrapy-redis

   1. 爬虫文件中

   ```python
   import scrapy
   from scrapy.linkextractors import LinkExtractor
   from scrapy.spiders import CrawlSpider, Rule
   # 1、导包
   from scrapy_redis.spiders import RedisCrawlSpider
   # 2、继承导包的内容
   class BqgSpider(RedisCrawlSpider):
       name = 'bqg'
       
       # 3、注销allowed_domains  和  start_urls
       # allowed_domains = ['www.com']
       # start_urls = ['https://www.xbiquge.la/0/885/']
       
       rules = (
           Rule(LinkExtractor(allow=r'^https://www.xbiquge.la/0/885/\d+.html$'), callback='parse_item', follow=False),
       )
       # 4、设置redis_key 数据库的key  后面需要任务
       redis_key = 'jiuge'  # 之后代码将会向redis中读取键名为jiuge 的值  来作为起始的url
       
       
       def parse_item(self, response):
           item = {}
           print(response.url)
           title = response.xpath('/html//div[@class="bookname"]/h1/text()').get()  # 获取章节
           coutent = response.xpath('//*[@id="content"]/text()').getall()
           item['title'] = title
           item['coutent'] = coutent
           print(item)
           return item
   ```

   > 步骤：
   >
   > 1、导包
   >
   > 2、继承
   >
   > 3、注释
   >
   > 4、设置redis_key

2. setting.py

   ```python
   # 编码格式的设置
   FEED_EXPORT_ENCODING = 'UTF-8'
   # 指定使用图片管道之后，图片保存的路径
   IMAGES_STORE = './'  # 项目的根目录
   
   # 过滤操作：             资源路径
   DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
   
   # 调度器：   资源路径(指定redis替换之后的调度器作用)
   SCHEDULER = "scrapy_redis.scheduler.Scheduler"
   # 调度器持久化 = True 开启任务持久化
   SCHEDULER_PERSIST = True
   
   # 管道
   ITEM_PIPELINES = {
      # 'scrapy__redis.pipelines.ScrapyRedisPipeline': 300,
       'scrapy_redis.pipelines.RedisPipeline':400  # 使用redis管道替换普通管道
   }
   
   # log信息等级
   LOG_LEVEL = 'DEBUG'
   
   # 下载延迟
   DOWNLOAD_DELAY = 0.5
   
   REDIS_URL = 'redis://192.168.1.150:6379'
   # REDIS_URL = 'redis://192.168.1.150:6379'
   # 连接的本地redis
   ```

   > 将之前的全部注释掉，把上面的代码粘贴

3. 运行

   ```
   1、在终端项目输入命令：scrapy      runspider       xxx.py（文件名）
   2、在cmd 中输入：
   	redis-cli      lpush     xxx（爬虫文件中写的redis_key）    路由url
   ```



### cmd操作

```python
cmd里面操作:命令
1.创建scrapy项目:(注意路径问题)
    -- scrapy startproject 项目名称xxx
        --    cd douban
              scrapy genspider example example.com

    -- 出现一个xxx项目名文件件:项目文件
            -- 1.项目同名的文件夹 : 处理scrapy核心代码
                -- spiders:爬虫组件: 定义起始url,提取数据
                -- __init__.py初始化文件,自带的一个起始文件,
                -- items.py:构建item对象,告诉引擎究竟是保存还是继续发送网络请求
                -- middlewares.py 中间件: 下载器中间件:用来做请求头参数的设置
                -- piplines.py: 管道,处理保存逻辑,本地(josn,txt,html)
                -- settings: 设置,参数的设置,作用

                -- scrapy自动完成的工作:调度器,引擎,下载器就没有具体的代码体现....

            -- 2.scrapy.cfg config缩写 : 以后做部署需要用到的配置文件
```















































































# 六、pyppeteer

```
安装
pip install pyppeteer -i https://pypi.douban.com/simple
```

[chromium下载地址：](https://npm.taobao.org/mirrors/chromium-browser-snapshots/)

### 1、简介

```python
Puppeteer 是 Google 基于 Node.js 开发的一个工具，有了它我们可以通过 JavaScript 来控制 Chrome 浏览器的一些操作，当然也可以用作网络爬虫上，其 API 极其完善，功能非常强大，Selenium 当然同样可以做到。

而 Pyppeteer 又是什么呢？它实际上是 Puppeteer 的 Python 版本的实现，但它不是 Google 开发的，是一位来自于日本的工程师依据 Puppeteer 的一些功能开发出来的非官方版本。
```

### 2、基本使用

```python
import asyncio
from pyppeteer import launch


async def main():
    # 浏览器 启动参数
    start_parm = {
        # 启动chrome的路径
        "executablePath": r"C:\Users\yq\AppData\Local\pyppeteer\pyppeteer\local-chromium\722234\chrome-win\chrome.exe",
        # 关闭无头浏览器 默认是无头启动的
        "headless": False,
    }
    # 创建浏览器对象，可以传入 字典形式参数
    browser = await launch(**start_parm)

    # 创建一个页面对象， 页面操作在该对象上执行
    page = await browser.newPage()

    await page.goto('https://www.httpbin.org/headers')  # 页面跳转
    page_text = await page.content()  # 页面内容
    print(page_text)

    await browser.close()  # 关闭浏览器对象
    
asyncio.get_event_loop().run_until_complete(main())   # 创建异步池并执行main函数。
```

> ignoreHTTPSErrors (bool): 是否要忽略 HTTPS 的错误，默认是 False。
> headless (bool): 是否启用 Headless 模式，即无界面模式，如果 devtools 这个参数是 True 的话，那么该参数就会被设置为 False，否则为 True，即默认是开启无界面模式的。
> executablePath (str): 可执行文件的路径，如果指定之后就不需要使用默认的 Chromium 了，可以指定为已有的 Chrome 或 Chromium。
> slowMo (int|float): 通过传入指定的时间，可以减缓 Pyppeteer 的一些模拟操作。
> args (List[str]): 在执行过程中可以传入的额外参数。
> ignoreDefaultArgs (bool): 不使用 Pyppeteer 的默认参数，如果使用了这个参数，那么最好通过 args 参数来设定一些参数，否则可能会出现一些意想不到的问题。这个参数相对比较危险，慎用。
> handleSIGINT (bool): 是否响应 SIGINT 信号，也就是可以使用 Ctrl + C 来终止浏览器程序，默认是 True。
> handleSIGTERM (bool): 是否响应 SIGTERM 信号，一般是 kill 命令，默认是 True。
> handleSIGHUP (bool): 是否响应 SIGHUP 信号，即挂起信号，比如终端退出操作，默认是 True。
> dumpio (bool): 是否将 Pyppeteer 的输出内容传给 process.stdout 和 process.stderr 对象，默认是 False。
> userDataDir (str): 即用户数据文件夹，即可以保留一些个性化配置和操作记录。
> env (dict): 环境变量，可以通过字典形式传入。
> devtools (bool): 是否为每一个页面自动开启调试工具，默认是 False。如果这个参数设置为 True，那么 headless 参数就会无效，会被强制设置为 False。
> logLevel (int|str): 日志级别，默认和 root logger 对象的级别相同。
> autoClose (bool): 当一些命令执行完之后，是否自动关闭浏览器，默认是 True。
> loop (asyncio.AbstractEventLoop): 时间循环对象。
>
> ```python
> import asyncio
> import logging
> import tkinter
> 
> from pyppeteer import launch, launcher
> from lxml import etree
> 
> 
> async def main():
> 
>     # 浏览器 启动参数
>     start_parm = {
>         # 启动chrome的路径
>         "executablePath": r"C:\Users\yq\AppData\Local\pyppeteer\pyppeteer\local-chromium\722234\chrome-win\chrome.exe",
>         # 关闭无头浏览器
>         "headless": False,
> 
>         "args": [
>             '--disable-infobars',  # 关闭自动化提示框
>             # '--window-size=1920,1080',  # 窗口大小
>             '--log-level=30',  # 日志保存等级， 建议设置越好越好，要不然生成的日志占用的空间会很大 30为warning级别
>             '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36',  # UA
>             '--no-sandbox',  # 关闭沙盒模式
>             '--start-maximized',  # 窗口最大化模式
>             # '--proxy-server=http://localhost:1080'  # 代理
>             r'userDataDir=D:\project_demo\python_demo\spider_demo\JavaScript 逆向系列课\userdata'  # 用户文件地址
>         ],
>     }
> 
>     await page.goto('https://www.httpbin.org/headers')
>     page_text = await page.content()
>     input('----------------')
>     await browser.close()
> 
> asyncio.get_event_loop().run_until_complete(main())
> ```

### 3、页面大小

```python
import asyncio
import tkinter

from pyppeteer import launcher

# 注意 在导入launch之前先把默认参数改了
# 去除自动化 启动参数
launcher.AUTOMATION_ARGS.remove("--enable-automation")
from pyppeteer import launch


async def main():
    # 浏览器 启动参数
    start_parm = {
        # 启动chrome的路径
        "executablePath": r"C:\Users\yq\AppData\Local\pyppeteer\pyppeteer\local-chromium\722234\chrome-win\chrome.exe",
        # 关闭无头浏览器
        "headless": False,
        "args": [
            '--disable-infobars',  # 关闭自动化提示框
            '--no-sandbox',  # 关闭沙盒模式
            '--start-maximized',  # 窗口最大化模式

        ],
    }
    browser = await launch(**start_parm)
    page = await browser.newPage()

    # 查看当前 桌面视图大小
    tk = tkinter.Tk()
    width = tk.winfo_screenwidth()
    height = tk.winfo_screenheight()
    tk.quit()
    print(f'设置窗口为：width：{width} height：{height}')
    
    # 设置网页 视图大小
    await page.setViewport(viewport={'width': width, 'height': height})
    await page.goto('https://www.baidu.com')
    await browser.close()


asyncio.get_event_loop().run_until_complete(main())
```

### 4、隐藏浏览器特征

pyppeteer跟selenium一样会有浏览器特征，所以需要修改，隐藏特征防止被识别。
主要有下面两点：

1. 去除浏览器自动化参数 `--enable-automation`
2. 去除`window.navigator.webdriver`等检测

```Python
import asyncio
import logging
import tkinter

from pyppeteer import launcher
# 第一步 去除浏览器自动化参数
# 必须在 from pyppeteer import launch 前去除参数
# 去除自动化 启动参数
launcher.AUTOMATION_ARGS.remove("--enable-automation")

from pyppeteer import launch
from lxml import etree


async def main():
    # 浏览器 启动参数
    start_parm = {
        # 启动chrome的路径
        "executablePath": r"C:\Users\yq\AppData\Local\pyppeteer\pyppeteer\local-chromium\722234\chrome-win\chrome.exe",
        # 关闭无头浏览器
        "headless": False,

        "args": [
            '--disable-infobars',  # 关闭自动化提示框
            # '--window-size=1920,1080',  # 窗口大小
            '--log-level=30',  # 日志保存等级， 建议设置越好越好，要不然生成的日志占用的空间会很大 30为warning级别
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36',  # UA
            '--no-sandbox',  # 关闭沙盒模式
            '--start-maximized',  # 窗口最大化模式
            # '--proxy-server=http://localhost:1080'  # 代理
            r'userDataDir=D:\project_demo\python_demo\spider_demo\JavaScript 逆向系列课\userdata'  # 用户文件地址
        ],
    }
    
    browser = await launch(**start_parm)
    page = await browser.newPage()
    tk = tkinter.Tk()
    width = tk.winfo_screenwidth()
    height = tk.winfo_screenheight()
    tk.quit()

    await page.setViewport(viewport={'width': width, 'height': height})
	
	# 第二步，修改 navigator.webdriver检测
	# 其实各种网站的检测js是不一样的，这是比较通用的。有的网站会检测运行的电脑运行系统，cpu核心数量，鼠标运行轨迹等等。
    # 反爬js
    js_text = """
() =>{ 
    Object.defineProperties(navigator,{ webdriver:{ get: () => false } });
    window.navigator.chrome = { runtime: {},  };
    Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
    Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5,6], });
 }
    """
    await page.evaluateOnNewDocument(js_text)  # 本页刷新后值不变，自动执行js
    await page.goto('https://www.httpbin.org/headers')
    page_text = await page.content()
    print(page_text)
    await browser.close()

asyncio.get_event_loop().run_until_complete(main())
```

### 5、拦截请求

```python
import asyncio
import json

from jsonpath import jsonpath
from pyppeteer import launcher

launcher.AUTOMATION_ARGS.remove("--enable-automation")

from pyppeteer import launch

from pyppeteer.network_manager import Request, Response


async def intercept_request(req:Request):
    await req.continue_()  # 请求，看源码可以重新编写请求


async def intercept_response(res:Response):
    if 'ext2020/apub/json/prevent.new' in res.url:
        print('拦截到请求')
        json_text = await res.text()
        title_li = jsonpath(json.loads(json_text), '$..title')
        for title in title_li:
            print(title)
    pass


async def main():
    # 浏览器 启动参数
    start_parm = {
        # 启动chrome的路径
        "executablePath": r"C:\Users\yq\AppData\Local\pyppeteer\pyppeteer\local-chromium\722234\chrome-win\chrome.exe",
        # 关闭无头浏览器 默认是无头启动的
        "headless": False,
        "args": [
            '--disable-infobars',  # 关闭自动化提示框
            # '--no-sandbox',  # 关闭沙盒模式
            '--start-maximized',  # 窗口最大化模式
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36',
            # UA

        ],

    }
    # 创建浏览器对象，可以传入 字典形式参数
    browser = await launch(**start_parm)

    # 创建一个页面对象， 页面操作在该对象上执行
    page = await browser.newPage()
    await page.setJavaScriptEnabled(enabled=True)

    # 启用拦截器
    await page.setRequestInterception(True)
    page.on('request', intercept_request) 
    page.on('response', intercept_response)
    
    js_text = """
    () =>{ 
        Object.defineProperties(navigator,{ webdriver:{ get: () => false } });
        window.navigator.chrome = { runtime: {},  };
        Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
        Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5,6], });
     }
        """
    await page.evaluateOnNewDocument(js_text)  # 本页刷新后值不变，自动执行js
    await page.goto('https://news.qq.com/')  # 页面跳转


    await browser.close()


asyncio.get_event_loop().run_until_complete(main())   # 创建异步池并执行main函数。
```

# 七、异步

```
import asyncio
import aiohttp   # 完成网络请求并发任务
```

### 1、

```
# 凡是被async修饰的函数都支持异步编程
async 函数（）：
	async with aiohttp.Cli
```

